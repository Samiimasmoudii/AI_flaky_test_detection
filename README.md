# AI_flaky_test_detection

## Flaky Test Detection with LLAMA 7B
Environment: GitHub Codespace

Machine Type: 4-core virtual machine (no GPU)

Model Used: llama-2-7b.Q4_K_M.gguf

Backend: CPU-only inference using llama.cpp-compatible loader

Use Case: Running quantized LLaMA 2 model for testing and lightweight inference in a CPU-only environment

### SETUPS INSTRCUTIONS 
chmod +x setup.sh
./setup.sh

